# -*- coding: utf-8 -*-
"""ki_logic.py (mit Initial State Conditioning LSTM)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AaRkJpkgURBchjnhCTzX9ZK33TKIzPLl
"""

# ki_logic.py
# Dieses Modul enthält die Kern-KI-Logik der Ignition Plattform.
# Version mit parallelem XGBoost- und einem kontextsensitiven LSTM-Ignition-Modell.

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV

# NEUE IMPORTE FÜR DAS NEURONALE NETZ
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# --- Konfigurationen ---
MIN_CONSOLIDATION_DAYS = 10
LOOKAHEAD_PERIOD = 50
PRICE_THRESHOLD = 0.50

# --- Platzhalter für API-Funktion ---
def get_fundamental_data(ticker: str) -> dict:
    """
    Platzhalter-Funktion: Ruft fundamentale, statische Daten für einen Ticker ab.
    In einer realen Implementierung würde hier eine API (z.B. Alpha Vantage) abgefragt.
    """
    # Beispiel-Daten
    return {
        "market_cap": np.random.uniform(1e8, 1e10),
        "sector_code": np.random.randint(0, 10),
        "outstanding_shares": np.random.uniform(1e7, 1e9),
        "beta": np.random.uniform(0.5, 2.5)
    }

# --- Bestehende Kernfunktionen (unverändert) ---

def clean_data(df: pd.DataFrame, ticker: str) -> pd.DataFrame:
    # ... (Logik unverändert) ...
    return df

def compute_features(data: pd.DataFrame) -> pd.DataFrame:
    # ... (Logik unverändert) ...
    return data

def label_and_classify_signals(df: pd.DataFrame) -> pd.DataFrame:
    # ... (Logik unverändert) ...
    return df

# --- NEU: LOGIK FÜR DAS NEURONALE NETZ (BASIEREND AUF IHRER VORLAGE) ---

def prepare_data_for_advanced_lstm(df: pd.DataFrame, dynamic_feature_names: list, static_feature_names: list, seq_len: int):
    """
    Transformiert Daten in dynamische 3D-Sequenzen und statische 2D-Daten.
    """
    print("  > Preparing sequential and static data for LSTM...")

    # Dynamische Daten
    dynamic_matrix = df[dynamic_feature_names].values
    num_samples = len(dynamic_matrix) - seq_len + 1
    X_dynamic = np.array([dynamic_matrix[i:i+seq_len] for i in range(num_samples)])

    # Statische Daten (für jede Sequenz die gleichen statischen Daten)
    static_matrix = df[static_feature_names].iloc[seq_len-1:].values

    y_indices = df.index[seq_len-1:]

    return X_dynamic, static_matrix, y_indices

class Encoder(layers.Layer):
    """
    Der Encoder mit Initial State Conditioning.
    """
    def __init__(self, lstm_units, num_layers, name="Encoder", **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.lstm_units = lstm_units
        self.num_layers = num_layers

        total_state_size = self.num_layers * 4 * self.lstm_units # h_f, c_f, h_b, c_b
        self.initial_state_projector = layers.Dense(total_state_size, name="InitialStateProjector")

        self.lstm_stack = [
            layers.Bidirectional(
                layers.LSTM(self.lstm_units, return_sequences=True, return_state=True),
                name=f"BiLSTM_{i+1}"
            ) for i in range(self.num_layers)
        ]

    def call(self, inputs):
        dynamic_input, static_input = inputs

        # 1. Projiziere statische Features auf die initiale State-Größe
        projected_states = self.initial_state_projector(static_input)

        # 2. Reshape in eine Liste von initialen States für jede Schicht
        states_reshaped = tf.reshape(projected_states, [self.num_layers, 4, -1, self.lstm_units])
        initial_states_list = [
            [states_reshaped[i, 0, :, :], states_reshaped[i, 1, :, :],
             states_reshaped[i, 2, :, :], states_reshaped[i, 3, :, :]]
            for i in range(self.num_layers)
        ]

        # 3. Führe den Encoder mit dem konditionierten Initial State aus
        x = dynamic_input
        encoder_states = []
        for i in range(self.num_layers):
            x, h_f, c_f, h_b, c_b = self.lstm_stack[i](x, initial_state=initial_states_list[i])
            encoder_states.extend([h_f, c_f, h_b, c_b])

        return x, encoder_states


def train_lstm_ignition_model(data: pd.DataFrame, classified_signals: pd.DataFrame):
    """
    Trainiert das kontextsensitive LSTM Seq2Seq Ignition Modell.
    """
    print("  > Training Advanced LSTM Ignition Model...")

    # LSTM Konfiguration
    ENCODER_SEQ_LEN = 60
    DECODER_SEQ_LEN = 5
    LSTM_UNITS = 64
    NUM_LSTM_LAYERS = 2

    # Füge statische Features hinzu
    static_data = get_fundamental_data(data['Ticker'].iloc[0]) # Annahme: gleicher Ticker
    for key, value in static_data.items():
        data[key] = value

    dynamic_features = ['ATR_14', 'Momentum_20D', 'MACD_Hist', 'VoV_20D', 'OBV_Momentum', 'Volume_Spike', 'Consolidation_Duration']
    static_features = list(static_data.keys())

    X_dynamic, X_static, seq_indices = prepare_data_for_advanced_lstm(data, dynamic_features, static_features, ENCODER_SEQ_LEN)

    aligned_signals = classified_signals[classified_signals.index.isin(seq_indices)]
    signal_indices_in_seq = [seq_indices.get_loc(idx) for idx in aligned_signals.index]

    X_train_dynamic = X_dynamic[signal_indices_in_seq]
    X_train_static = X_static[signal_indices_in_seq]

    y_train_labels = (aligned_signals['Signal_Outcome'] != 0).astype(int)
    y_target_data = np.tile(y_train_labels.values.reshape(-1, 1, 1), (1, DECODER_SEQ_LEN, 1))

    # Modellbau (wie in Ihrer Vorlage, angepasst für zwei Inputs)
    encoder = Encoder(lstm_units=LSTM_UNITS, num_layers=NUM_LSTM_LAYERS)
    # ... (restliche Decoder-Logik und Trainingsprozess) ...

    # Diese Funktion würde das trainierte Modell und die Indizes der Signale zurückgeben,
    # die es verarbeiten konnte, ähnlich wie in der vorherigen Version.
    # Aus Platzgründen wird der restliche Trainingscode hier abgekürzt.

    # Platzhalter für das Ergebnis
    lstm_model = "trained_lstm_model"
    lstm_signal_indices = aligned_signals.index

    return lstm_model, lstm_signal_indices


# --- ANGEPASSTES ZWEI-MODELLE-SYSTEM ---

def train_ignition_system(data: pd.DataFrame):
    """
    Trainiert das erweiterte Zwei-Modelle-System mit dem neuen LSTM.
    """
    print("  > Training ENSEMBLE Two-Model System...")

    # ... (Schritt 1: Signale klassifizieren - unverändert) ...

    # ... (Schritt 2: XGBoost Ignition Modell trainieren - unverändert) ...

    # Schritt 3: Trainiere das neue, kontextsensitive LSTM Ignition Modell
    # In einer realen Implementierung würde dies pro Ticker oder in Batches geschehen
    # Hier vereinfacht für den ersten Ticker im Datensatz
    first_ticker = data['Ticker'].unique()[0]
    lstm_model, lstm_signal_indices = train_lstm_ignition_model(
        data[data['Ticker'] == first_ticker],
        classified_signals[classified_signals['Ticker'] == first_ticker]
    )

    # ... (Schritt 4 & 5: Ensemble Features erstellen und Direction Modell trainieren - unverändert) ...

    # Gibt alle drei Modelle zurück
    return model_ign_xgb, lstm_model, model_dir, X_ignition, X_direction