# -*- coding: utf-8 -*-
"""Ignition-Wert Framework (Final & Corrected)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rFKMWgACuHGovg-D72csCGUctpai5l6
"""

# Install necessary packages (run this cell first in Colab)
!pip install flask flask-cors xgboost torch pandas numpy scikit-learn google-cloud-storage pyngrok joblib optuna flask-socketio eventlet -q

# Import libraries
from flask import Flask, request, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import os
import json
import time
import hashlib
import tempfile
import csv
from datetime import datetime
from werkzeug.utils import secure_filename
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import random
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from pyngrok import ngrok
import joblib
from google.cloud import storage
import optuna
import eventlet

# WICHTIG: Notwendig, damit SocketIO korrekt mit Standardbibliotheken funktioniert
eventlet.monkey_patch()

# ==============================================================================
# 1. Konfiguration und Initialisierung
# ==============================================================================

# GCSStorage Klasse (real, mit Credentials)
class GCSStorage:
    def __init__(self):
        self.project_id = "ignition-ki-csv-storage"
        self.bucket_name = "ignition-ki-csv-data-2025-user123"
        credentials_path = 'google-credentials.json'  # Annahme: Datei liegt im Colab-Root
        if os.path.exists(credentials_path):
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
            try:
                self.client = storage.Client(project=self.project_id)
                self.bucket = self.client.bucket(self.bucket_name)
                print(f"✅ GCS initialisiert: {self.bucket_name}")
            except Exception as e:
                print(f"❌ GCS Fehler: {str(e)}. Fallback zu lokalem Speicher.")
                self.bucket = None
        else:
            print("⚠️ Google Credentials nicht gefunden. GCS ist deaktiviert.")
            self.bucket = None

    def save_ticker_data(self, ticker, data_dict):
        if not self.bucket:
            return False
        try:
            file_data = {'ticker': ticker, 'data': data_dict, 'timestamp': datetime.now().isoformat(), 'data_points': len(data_dict)}
            blob_name = f"tickers/{ticker}.json"
            blob = self.bucket.blob(blob_name)
            blob.upload_from_string(json.dumps(file_data, indent=2), content_type='application/json')
            print(f"✅ Ticker {ticker} in GCS gespeichert.")
            return True
        except Exception as e:
            print(f"❌ Fehler beim Speichern von {ticker}: {str(e)}")
            return False

gcs_storage = GCSStorage()

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})
socketio = SocketIO(app, async_mode='eventlet')

# Zentrale Hyperparameter
HPARAMS = {
    'input_dim': 7,  # 5 Basis-Features + 2 erzeugte Features
    'hidden_dim': 128,
    'n_layers': 2,
    'dropout_prob': 0.2
}

# ==============================================================================
# 2. LSTM Seq2Seq Modellarchitektur
# ==============================================================================
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        # KORRIGIERT: self.n_layers muss hier initialisiert werden
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, src):
        outputs, (hidden, cell) = self.rnn(src)
        hidden = hidden.view(self.n_layers, 2, -1, self.hidden_dim)
        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)
        hidden = self.dropout(hidden)
        cell = cell.view(self.n_layers, 2, -1, self.hidden_dim)
        cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)
        cell = self.dropout(cell)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.rnn = nn.LSTM(output_dim, hidden_dim * 2, n_layers, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(1)
        output, (hidden, cell) = self.rnn(input, (hidden, cell))
        prediction = self.fc_out(self.dropout(output.squeeze(1)))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len, trg_dim = src.shape[0], trg.shape[1], trg.shape[2]
        outputs = torch.zeros(batch_size, trg_len, trg_dim).to(self.device)
        hidden, cell = self.encoder(src)
        input = trg[:, 0, :]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t, :] = output
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[:, t, :] if teacher_force else output
        return outputs

    def predict(self, src, prediction_length=30):
        self.eval()
        with torch.no_grad():
            batch_size, trg_dim = src.shape[0], src.shape[2]
            outputs = torch.zeros(batch_size, prediction_length, trg_dim).to(self.device)
            hidden, cell = self.encoder(src)
            input = src[:, -1, :]
            for t in range(prediction_length):
                output, hidden, cell = self.decoder(input, hidden, cell)
                outputs[:, t, :] = output
                input = output
        return outputs

# ==============================================================================
# 3. Hilfsfunktionen (Loss, Datenvorbereitung, Signallogik)
# ==============================================================================
class AsymmetricWeightedMSELoss(nn.Module):
    def __init__(self, catastrophe_penalty=10.0, correct_breakout_bonus=2.0):
        super().__init__()
        self.catastrophe_penalty = catastrophe_penalty
        self.correct_breakout_bonus = correct_breakout_bonus

    def forward(self, predictions, targets, upper_bound, lower_bound):
        base_mse = (predictions - targets) ** 2
        weights = torch.ones_like(base_mse)
        catastrophe_condition = (predictions > upper_bound) & (targets < lower_bound)
        weights[catastrophe_condition] = self.catastrophe_penalty
        correct_breakout_condition = (predictions > upper_bound) & (targets > upper_bound)
        weights[correct_breakout_condition] = self.correct_breakout_bonus
        weighted_mse = weights * base_mse
        return torch.mean(weighted_mse)

def _create_features(df):
    """Hilfsfunktion zur konsistenten Erzeugung von Features."""
    df['range'] = df['high'] - df['low']
    df['avg_price'] = (df['high'] + df['low']) / 2
    return df

def prepare_sequences(csv_path, window_size=75, prediction_length=30, step=1):
    """Bereitet Trainingsdaten mit einem Sliding-Window-Ansatz vor."""
    df = pd.read_csv(csv_path)
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    if not all(col in df.columns for col in required_columns):
        raise ValueError('CSV fehlen benötigte Spalten')

    df = _create_features(df)

    scaler = MinMaxScaler()
    feature_cols = required_columns + ['range', 'avg_price']
    data = scaler.fit_transform(df[feature_cols])

    src_sequences, trg_sequences = [], []
    for i in range(0, len(data) - window_size - prediction_length + 1, step):
        src_sequences.append(data[i:i + window_size])
        trg_sequences.append(data[i + window_size:i + window_size + prediction_length])

    return np.array(src_sequences), np.array(trg_sequences), scaler

def prepare_inference_from_df(df, scaler, window_size=75):
    """Bereitet die letzte Sequenz aus einem DataFrame für die Vorhersage vor."""
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    if not all(col in df.columns for col in required_columns):
        raise ValueError('DataFrame fehlen benötigte Spalten')

    df_copy = _create_features(df.copy())

    feature_cols = required_columns + ['range', 'avg_price']
    data = scaler.transform(df_copy[feature_cols])

    if len(data) < window_size:
        return np.array([])

    src_sequence = data[-window_size:]
    return np.array([src_sequence])

def identify_consolidation(close_prices, window=20, squeeze_period=120, squeeze_percentile=0.10):
    """
    VERBESSERT: Identifiziert eine Konsolidierungsphase mittels eines Bollinger Band Squeeze.
    """
    if len(close_prices) < squeeze_period:
        return False

    middle_band = close_prices.rolling(window).mean()
    std_dev = close_prices.rolling(window).std()
    upper_band = middle_band + (2 * std_dev)
    lower_band = middle_band - (2 * std_dev)
    bandwidth = (upper_band - lower_band) / middle_band
    current_bandwidth = bandwidth.iloc[-1]
    historical_bandwidth = bandwidth.iloc[-squeeze_period:]
    squeeze_threshold = historical_bandwidth.quantile(squeeze_percentile)
    return current_bandwidth < squeeze_threshold

# ==============================================================================
# 4. Modell-Trainingsfunktionen
# ==============================================================================
def train_lstm(csv_path):
    device = torch.device('cpu')
    src, trg, scaler = prepare_sequences(csv_path)
    if src.shape[0] == 0:
        print("Nicht genügend Daten für LSTM-Training.")
        return None, None, None

    input_dim = src.shape[-1]

    encoder = Encoder(input_dim, HPARAMS['hidden_dim'], HPARAMS['n_layers'], HPARAMS['dropout_prob']).to(device)
    decoder = Decoder(input_dim, HPARAMS['hidden_dim'], HPARAMS['n_layers'], HPARAMS['dropout_prob']).to(device)
    model = Seq2Seq(encoder, decoder, device).to(device)

    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = AsymmetricWeightedMSELoss()

    src_tensor = torch.tensor(src, dtype=torch.float32).to(device)
    trg_tensor = torch.tensor(trg, dtype=torch.float32).to(device)

    last_known_close = src_tensor[:, -1, 3]
    historical_volatility = src_tensor[:, :, 3].std(dim=1)
    upper_bound_val = last_known_close + (2 * historical_volatility)
    lower_bound_val = last_known_close - (2 * historical_volatility)
    upper_bound = upper_bound_val.unsqueeze(1).unsqueeze(2).expand_as(trg_tensor)
    lower_bound = lower_bound_val.unsqueeze(1).unsqueeze(2).expand_as(trg_tensor)

    dataset = TensorDataset(src_tensor, trg_tensor, upper_bound, lower_bound)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    model.train()
    for epoch in range(5):
        epoch_loss = 0
        for src_batch, trg_batch, upper_batch, lower_batch in loader:
            output = model(src_batch, trg_batch)
            loss = criterion(output[:, 1:, :], trg_batch[:, 1:, :], upper_batch[:, 1:, :], lower_batch[:, 1:, :])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(loader)
        print(f"LSTM Training Epoche {epoch+1}, Loss: {avg_loss:.6f}")
        socketio.emit('training_progress', {'model': 'LSTM', 'epoch': epoch + 1, 'loss': avg_loss})

    model_version = datetime.now().strftime("%Y%m%d%H%M%S")
    torch.save(model.state_dict(), f'lstm_model_v{model_version}.pth')
    joblib.dump(scaler, f'lstm_scaler_v{model_version}.pkl')
    return model, scaler, model_version

def train_xgboost(csv_path):
    df = pd.read_csv(csv_path)
    df = _create_features(df)

    feature_cols = ['open', 'high', 'low', 'volume', 'range', 'avg_price']
    features = df[feature_cols].values
    targets = df['close'].values

    if len(features) < 5:
        print("Nicht genügend Daten für XGBoost-Training.")
        return None, None

    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)
    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)
    model.fit(X_train, y_train)

    model_version = datetime.now().strftime("%Y%m%d%H%M%S")
    joblib.dump(model, f'xgboost_model_v{model_version}.pkl')
    return model, model_version

# ==============================================================================
# 5. Hauptausführung: Training und Laden der Modelle
# ==============================================================================
# Erzeuge eine Dummy-CSV mit mehr Daten für robustes Training
df = pd.DataFrame({
    'open': np.linspace(100, 150, 300) + np.random.randn(300) * 2,
    'high': np.linspace(102, 155, 300) + np.random.randn(300) * 2,
    'low': np.linspace(98, 148, 300) + np.random.randn(300) * 2,
    'close': np.linspace(101, 152, 300) + np.random.randn(300) * 2,
    'volume': np.linspace(10000, 50000, 300) + np.random.randn(300) * 1000
})
df.to_csv('test.csv', index=False)

# Führe das Training für beide Modelle aus
print("--- Training der Modelle ---")
lstm_model, lstm_scaler, lstm_version = train_lstm('test.csv')
xgboost_model, xgboost_version = train_xgboost('test.csv')
print("\nBeide Modelle wurden trainiert und gespeichert.")

# Lade die trainierten Modelle und den Scaler für die Inferenz
print("\n--- Lade Modelle für die Inferenz ---")
device = torch.device('cpu')
encoder = Encoder(HPARAMS['input_dim'], HPARAMS['hidden_dim'], HPARAMS['n_layers'], HPARAMS['dropout_prob']).to(device)
decoder = Decoder(HPARAMS['input_dim'], HPARAMS['hidden_dim'], HPARAMS['n_layers'], HPARAMS['dropout_prob']).to(device)
trained_lstm = Seq2Seq(encoder, decoder, device).to(device)
trained_lstm.load_state_dict(torch.load(f'lstm_model_v{lstm_version}.pth'))
trained_lstm.eval()
lstm_scaler = joblib.load(f'lstm_scaler_v{lstm_version}.pkl')
trained_xgboost = joblib.load(f'xgboost_model_v{xgboost_version}.pkl')
print("Modelle erfolgreich geladen.")

# ==============================================================================
# 6. Flask API Routen
# ==============================================================================
def calculate_breakout_signal(lstm_preds, ignition_value, current_close, df):
    is_consolidating = identify_consolidation(df['close'])

    projected_close = lstm_preds[-1][3]  # Index 3 ist 'close'
    projected_gain = (projected_close - current_close) / current_close * 100 if current_close > 0 else 0

    signal = "NO_SIGNAL"
    if is_consolidating:
        if ignition_value > current_close and projected_gain >= 30:
            signal = "STRONG_BULLISH_BREAKOUT"
        elif ignition_value < current_close and projected_gain <= -30:
            signal = "STRONG_BEARISH_BREAKOUT"
        else:
            signal = "CONSOLIDATING_NO_BREAKOUT"

    return signal, projected_gain

@app.route('/predict', methods=['POST'])
def predict_breakout():
    if 'file' not in request.files:
        return jsonify({'error': 'Keine Datei im Request gefunden'}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Keine Datei ausgewählt'}), 400

    if file and file.filename.endswith('.csv'):
        try:
            # --- Direkte und sichere Verarbeitung des DataFrames ---
            df = pd.read_csv(file.stream)
        except Exception as e:
            return jsonify({'error': f'Fehler beim Lesen der CSV-Datei: {str(e)}'}), 400

        # --- Spaltenvalidierung ---
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_columns):
            return jsonify({'error': f'CSV fehlen benötigte Spalten: {required_columns}'}), 400

        # --- GCS Integration ---
        ticker = file.filename.split('.')[0]
        gcs_storage.save_ticker_data(ticker, df.to_dict(orient='records'))

        # --- LSTM Vorhersage ---
        try:
            sequences = prepare_inference_from_df(df, lstm_scaler)
        except ValueError as e:
             return jsonify({'error': f'Fehler bei der Datenvorbereitung: {e}'}), 400

        if sequences.size == 0:
            return jsonify({'error': 'Nicht genügend Daten in der Datei für eine Vorhersage.'}), 400

        src_tensor_pred = torch.tensor(sequences, dtype=torch.float32).to(device)

        with torch.no_grad():
            lstm_preds = trained_lstm.predict(src_tensor_pred)

        # Die einfache inverse_transform ist korrekt, da das Modell alle 7 Features vorhersagt
        unscaled_lstm_preds = lstm_scaler.inverse_transform(lstm_preds.squeeze(0).cpu().numpy())

        # --- XGBoost Vorhersage (parallel) ---
        df_with_features = _create_features(df.copy())
        feature_cols = ['open', 'high', 'low', 'volume', 'range', 'avg_price']
        features = df_with_features[feature_cols].values[-1:]
        xgboost_pred = trained_xgboost.predict(features)[0]

        # --- Signal kombinieren ---
        current_close = df['close'].iloc[-1]
        signal, projected_gain = calculate_breakout_signal(unscaled_lstm_preds, xgboost_pred, current_close, df)

        response_data = {
            'lstm_prediction_30_steps': unscaled_lstm_preds.tolist(),
            'xgboost_ignition_value': float(xgboost_pred),
            'signal': signal,
            'projected_gain_percent': projected_gain,
            'current_close': current_close
        }
        return jsonify(response_data)

    return jsonify({'error': 'Ungültiger Dateityp, bitte eine .csv-Datei hochladen'}), 400

@app.route('/')
def index():
    return "LSTM & XGBoost Prediction Server is running!"

# ==============================================================================
# 7. Server Start
# ==============================================================================
# Führe Flask mit Ngrok aus
try:
    ngrok.kill()
except Exception as e:
    print(f"Konnte ngrok-Tunnel nicht beenden: {e}")

public_url = ngrok.connect(5000)
print(f" * Public URL: {public_url}")
print(" * Senden Sie eine POST-Anfrage mit einer CSV-Datei an /predict")

# Starte den Server mit socketio.run()
if __name__ == '__main__':
    # Wir wrappen den App-Start, damit die Colab-Zelle nicht blockiert
    import threading

    def run_app():
        # eventlet als Webserver für SocketIO verwenden
        import eventlet
        eventlet.wsgi.server(eventlet.listen(('', 5000)), app)

    flask_thread = threading.Thread(target=run_app)
    flask_thread.daemon = True
    flask_thread.start()
    print(" * Flask-Server läuft in einem Hintergrund-Thread.")