<DOCUMENT filename="ignition_wert_framework_(produktionsreife_version).py">
# -*- coding: utf-8 -*-
"""Ignition-Wert Framework (Produktionsreife Version)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/107Ftc8xBkrtZUOWfGRbXyp4KEIR3Jpz9
"""

# ==============================================================================
# 1. Installation und Importe
# ==============================================================================
# Die Option -q wurde entfernt, um Installationsdetails sichtbar zu machen.
!pip install flask flask-cors xgboost torch pandas numpy scikit-learn google-cloud-storage pyngrok joblib optuna flask-socketio eventlet backtrader celery redis pandas_ta

# Standardbibliotheken
import os
import json
import time
import tempfile
from datetime import datetime
import threading
import random
from functools import lru_cache
import traceback

# Installierte Bibliotheken
import numpy as np
import pandas as pd
import joblib
from werkzeug.utils import secure_filename
from celery.result import AsyncResult

# Machine Learning Bibliotheken
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
import pandas_ta as ta

# Webserver und Cloud
from flask import Flask, request, jsonify, abort
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from pyngrok import ngrok
from google.cloud import storage
import eventlet

# Backtesting
import backtrader as bt

# Async Tasks
from celery import Celery

# WICHTIG: eventlet.monkey_patch() ist für die asynchrone Funktionalität von SocketIO
# in Kombination mit Standardbibliotheken wie 'requests' (indirekt von GCS genutzt)
# notwendig. Es muss vor fast allen anderen Imports ausgeführt werden.
eventlet.monkey_patch()


# ==============================================================================
# 2. Konfiguration
# ==============================================================================
class Config:
    """Zentrale Konfigurationsklasse zur Vermeidung von hartcodierten Werten."""
    # GCS Konfiguration
    GCS_PROJECT_ID = os.environ.get("GCS_PROJECT_ID", "ignition-ki-csv-storage")
    GCS_BUCKET_NAME = os.environ.get("GCS_BUCKET_NAME", "ignition-ki-csv-data-2025-user123")
    GCS_CREDENTIALS_PATH = 'google-credentials.json'

    # Webserver Konfiguration
    ALLOWED_CORS_ORIGINS = os.environ.get("ALLOWED_CORS_ORIGINS", "*")

    # Modell-Hyperparameter (Standardwerte, können durch Optuna optimiert werden)
    LSTM_HPARAMS = {
        'input_dim': 11,  # 5 Basis + 6 erzeugte Features
        'hidden_dim': 128,
        'n_layers': 2,
        'dropout_prob': 0.25,
        'learning_rate': 0.001,
        'epochs': 50
    }
    XGBOOST_HPARAMS = {
        'objective': 'reg:squarederror',
        'n_estimators': 150,
        'learning_rate': 0.05,
        'max_depth': 5,
        'subsample': 0.8
    }

    # Datenvorbereitung
    DATA_WINDOW_SIZE = 75
    DATA_PREDICTION_LENGTH = 30

    # Konsolidierungserkennung
    CONSOLIDATION_BB_WINDOW = 20
    CONSOLIDATION_LOOKBACK_PERIOD = 50
    CONSOLIDATION_CHANNEL_THRESHOLD = 0.05

    # Modell-Registrierung
    MODEL_REGISTRY_PATH = 'model_registry.json'

    # Backtesting
    BACKTEST_INITIAL_CASH = 100000
    BACKTEST_COMMISSION = 0.001

    # API
    MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB

    # Celery
    CELERY_BROKER_URL = os.environ.get('CELERY_BROKER_URL', 'redis://localhost:6379/0')
    CELERY_RESULT_BACKEND = os.environ.get('CELERY_RESULT_BACKEND', 'redis://localhost:6379/0')

    # Signal
    ATR_MULTIPLIER = 2.5
    ADX_THRESHOLD = 25


# ==============================================================================
# 3. GCS-Integration und Modell-Registrierung
# ==============================================================================
class GCSStorage:
    """Verwaltet die Speicherung und das Laden von Daten in Google Cloud Storage."""
    def __init__(self, project_id, bucket_name, credentials_path):
        if os.path.exists(credentials_path):
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
            try:
                self.client = storage.Client(project=project_id)
                self.bucket = self.client.bucket(bucket_name)
                if not self.bucket.exists():
                    self.bucket.create()
            except Exception as e:
                self.bucket = None
        else:
            self.bucket = None

    def save_ticker_data(self, ticker, data_dict):
        if not self.bucket: return False
        try:
            blob = self.bucket.blob(f"data/{ticker}/{datetime.now().strftime('%Y-%m-%d')}.json")
            blob.upload_from_string(json.dumps(data_dict, indent=2), content_type='application/json')
            return True
        except Exception: return False

def update_model_registry(ticker, model_type, model_path):
    """Aktualisiert die zentrale Modell-Registrierung atomar."""
    registry = {}
    if os.path.exists(Config.MODEL_REGISTRY_PATH):
        with open(Config.MODEL_REGISTRY_PATH, 'r') as f:
            registry = json.load(f)
    if ticker not in registry:
        registry[ticker] = {}
    registry[ticker][model_type] = model_path
    registry[ticker]['timestamp'] = datetime.now().isoformat()
    with open(Config.MODEL_REGISTRY_PATH, 'w') as f:
        json.dump(registry, f, indent=2)

# Initialisiere GCS-Instanz
gcs_storage = GCSStorage(Config.GCS_PROJECT_ID, Config.GCS_BUCKET_NAME, Config.GCS_CREDENTIALS_PATH)


# ==============================================================================
# 4. Webserver- und Celery-Initialisierung
# ==============================================================================
app = Flask(__name__)
app.config.update(
    CELERY_BROKER_URL=Config.CELERY_BROKER_URL,
    CELERY_RESULT_BACKEND=Config.CELERY_RESULT_BACKEND
)
CORS(app, resources={r"/*": {"origins": Config.ALLOWED_CORS_ORIGINS}})
socketio = SocketIO(app, async_mode='eventlet', message_queue=Config.CELERY_BROKER_URL)
celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL'])
celery.conf.update(app.config)


# ==============================================================================
# 5. LSTM Seq2Seq Modellarchitektur
# ==============================================================================
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.dropout = nn.Dropout(dropout_prob)
    def forward(self, src):
        _, (hidden, cell) = self.rnn(src)
        hidden = hidden.view(self.n_layers, 2, -1, self.hidden_dim)
        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)
        cell = cell.view(self.n_layers, 2, -1, self.hidden_dim)
        cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)
        return self.dropout(hidden), self.dropout(cell)

class Decoder(nn.Module):
    def __init__(self, output_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.rnn = nn.LSTM(output_dim, hidden_dim * 2, n_layers, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout_prob)
    def forward(self, input, hidden, cell):
        input = input.unsqueeze(1)
        output, (hidden, cell) = self.rnn(input, (hidden, cell))
        return self.fc_out(self.dropout(output.squeeze(1))), hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder, self.decoder, self.device = encoder, decoder, device
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len, trg_dim = src.shape[0], trg.shape[1], trg.shape[2]
        outputs = torch.zeros(batch_size, trg_len, trg_dim).to(self.device)
        hidden, cell = self.encoder(src)
        input = trg[:, 0, :]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t, :] = output
            if random.random() < teacher_forcing_ratio:
                input = trg[:, t, :]
            else:
                input = output
        return outputs
    def predict(self, src, prediction_length=30):
        self.eval()
        with torch.no_grad():
            batch_size, trg_dim = src.shape[0], src.shape[2]
            outputs = torch.zeros(batch_size, prediction_length, trg_dim).to(self.device)
            hidden, cell = self.encoder(src)
            input = src[:, -1, :]
            for t in range(prediction_length):
                output, hidden, cell = self.decoder(input, hidden, cell)
                outputs[:, t, :] = output
                input = output
        return outputs


# ==============================================================================
# 6. Hilfsfunktionen (Loss, Datenvorbereitung, Signallogik)
# ==============================================================================
class AsymmetricWeightedMSELoss(nn.Module):
    def __init__(self, catastrophe_penalty=10.0, correct_breakout_bonus=2.0):
        super().__init__()
        self.catastrophe_penalty = catastrophe_penalty
        self.correct_breakout_bonus = correct_breakout_bonus
    def forward(self, predictions, targets, upper_bound, lower_bound):
        base_mse = (predictions - targets) ** 2
        weights = torch.ones_like(base_mse)
        weights[(predictions > upper_bound) & (targets < lower_bound)] = self.catastrophe_penalty
        weights[(predictions > upper_bound) & (targets > upper_bound)] = self.correct_breakout_bonus
        return torch.mean(weights * base_mse)

def _create_features(df):
    df_copy = df.copy()
    df_copy.ta.rsi(append=True)
    df_copy.ta.macd(append=True)
    df_copy.ta.atr(append=True)
    df_copy.ta.adx(append=True)
    df_copy['range'] = df_copy['high'] - df_copy['low']
    df_copy['avg_price'] = (df_copy['high'] + df_copy['low']) / 2
    # KORRIGIERT: Korrekte Reihenfolge zur Vermeidung von Data Leakage
    df_copy.fillna(method='ffill', inplace=True)
    df_copy.fillna(method='bfill', inplace=True)
    return df_copy

def get_feature_columns():
    return ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'RSI_14', 'MACD_12_26_9', 'ATRr_14', 'ADX_14']

def prepare_sequences(df, window_size, prediction_length, step=1, scaler=None):
    df_with_features = _create_features(df)
    feature_cols = get_feature_columns()
    
    # KORRIGIERT: Logik zur Handhabung des Scalers
    if scaler is None:
        scaler = MinMaxScaler()
        data = scaler.fit_transform(df_with_features[feature_cols])
    else:
        data = scaler.transform(df_with_features[feature_cols])
        
    src, trg = [], []
    for i in range(len(data) - window_size - prediction_length + 1):
        src.append(data[i:i + window_size])
        trg.append(data[i + window_size:i + window_size + prediction_length])
    return np.array(src), np.array(trg), scaler

def prepare_inference_from_df(df, scaler, window_size):
    df_with_features = _create_features(df)
    if len(df_with_features) < window_size: return np.array([])
    feature_cols = get_feature_columns()
    data = scaler.transform(df_with_features[feature_cols])
    return np.array([data[-window_size:]])

def identify_consolidation(df, price_col='close', window=20, lookback_period=50, channel_threshold=0.05):
    if len(df) < lookback_period: return False, None, None
    middle = df[price_col].rolling(window).mean()
    std = df[price_col].rolling(window).std()
    bandwidth = (middle + 2 * std) - (middle - 2 * std) / middle
    is_squeeze = bandwidth.iloc[-1] < bandwidth.rolling(lookback_period).quantile(0.10).iloc[-1]
    if not is_squeeze: return False, None, None
    recent = df.iloc[-lookback_period:]
    high, low = recent['high'].max(), recent['low'].min()
    outliers = recent[(recent[price_col] > high) | (recent[price_col] < low)]
    is_valid = (len(outliers) / lookback_period) <= channel_threshold
    adx = df['ADX_14'].iloc[-1] if 'ADX_14' in df.columns else 0
    if is_squeeze and is_valid and adx < Config.ADX_THRESHOLD:
        return True, low, high
    return False, None, None