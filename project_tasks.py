{"cells":[{"cell_type":"code","source":"# project/tasks.py\nimport pandas as pd\nimport logging\nfrom datetime import datetime\nimport optuna\n\n# Import from other project modules\nfrom .extensions import celery\nfrom .training import build_and_train_lstm, build_and_train_xgboost\nfrom .features import get_feature_columns\nfrom .storage import get_gcs_storage\nfrom .tuning import objective_lstm, objective_xgboost\nfrom .config import Config\n\nlogger = logging.getLogger(__name__)\n\n@celery.task(bind=True)\ndef tune_and_train_async(self, df_json, model_type, ticker):\n    \"\"\"\n    Führt zuerst Hyperparameter-Tuning mit Optuna durch und trainiert dann\n    das finale Modell mit den besten gefundenen Parametern.\n    \"\"\"\n    try:\n        self.update_state(state='PROGRESS', meta={'status': 'Daten werden geladen...'})\n        df = pd.read_json(df_json, orient='split')\n        \n        if model_type == 'lstm':\n            objective_func = objective_lstm\n            build_func = build_and_train_lstm\n        elif model_type == 'xgboost':\n            objective_func = objective_xgboost\n            build_func = build_and_train_xgboost\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        # --- Tuning-Phase ---\n        self.update_state(state='PROGRESS', meta={'status': f'Starte Optuna-Tuning für {model_type}...'})\n        study = optuna.create_study(direction='minimize')\n        study.optimize(lambda trial: objective_func(trial, df), n_trials=Config.OPTUNA_N_TRIALS, timeout=Config.OPTUNA_TIMEOUT)\n        \n        best_hparams = study.best_params\n        logger.info(f\"Optuna tuning finished. Best hyperparameters for {model_type}: {best_hparams}\")\n\n        # --- Trainings-Phase mit den besten Parametern ---\n        self.update_state(state='PROGRESS', meta={'status': f'Trainiere finales {model_type}-Modell...'})\n        \n        # KORREKTUR: Der Aufruf der Trainingsfunktion muss die unterschiedlichen\n        # Argumente für LSTM (mit selected_features) und XGBoost (ohne) berücksichtigen.\n        if model_type == 'lstm':\n            best_hparams['epochs'] = Config.LSTM_HPARAMS['epochs']\n            final_result = build_func(df, get_feature_columns(), best_hparams)\n        else: # xgboost\n            final_result = build_func(df, best_hparams)\n        \n        # --- Speicher-Phase ---\n        self.update_state(state='PROGRESS', meta={'status': 'Speichere optimiertes Modell in GCS...'})\n        gcs = get_gcs_storage()\n        if gcs.client:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            if model_type == 'lstm':\n                model_path = f\"models/{ticker}/lstm_tuned_{timestamp}.pth\"\n                scaler_path = f\"models/{ticker}/scaler_tuned_{timestamp}.pkl\"\n                gcs.upload_pytorch_model(final_result['model'].state_dict(), model_path)\n                gcs.upload_joblib(final_result['scaler'], scaler_path)\n            else: # xgboost\n                model_path = f\"models/{ticker}/xgboost_tuned_{timestamp}.joblib\"\n                gcs.upload_joblib(final_result['model'], model_path)\n        \n        return {'status': 'SUCCESS', 'ticker': ticker, 'best_params': best_hparams}\n\n    except Exception as e:\n        logger.error(f\"Tune-and-train task failed: {e}\")\n        return {'status': 'FAILED', 'error': str(e)}\n\n@celery.task(bind=True)\ndef train_lstm_async(self, df_json, hparams, ticker):\n    \"\"\"\n    KORRIGIERT: Führt ein schnelles Training des LSTM-Modells mit den\n    Default-Hyperparametern aus der Config-Datei durch.\n    \"\"\"\n    try:\n        self.update_state(state='PROGRESS', meta={'status': 'Daten werden geladen...'})\n        df = pd.read_json(df_json, orient='split')\n        \n        self.update_state(state='PROGRESS', meta={'status': 'Training des LSTM-Modells mit Default-Parametern...'})\n        result = build_and_train_lstm(df, get_feature_columns(), hparams)\n        \n        self.update_state(state='PROGRESS', meta={'status': 'Speichere Modell in GCS...'})\n        gcs = get_gcs_storage()\n        if gcs.client:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            model_path = f\"models/{ticker}/lstm_{timestamp}.pth\"\n            scaler_path = f\"models/{ticker}/scaler_{timestamp}.pkl\"\n            gcs.upload_pytorch_model(result['model'].state_dict(), model_path)\n            gcs.upload_joblib(result['scaler'], scaler_path)\n        \n        return {'status': 'SUCCESS', 'ticker': ticker}\n    except Exception as e:\n        logger.error(f\"LSTM training task failed: {e}\")\n        return {'status': 'FAILED', 'error': str(e)}\n\n@celery.task(bind=True)\ndef train_xgboost_async(self, df_json, hparams, ticker):\n    \"\"\"\n    KORRIGIERT: Führt ein schnelles Training des XGBoost-Modells mit den\n    Default-Hyperparametern aus der Config-Datei durch.\n    \"\"\"\n    try:\n        self.update_state(state='PROGRESS', meta={'status': 'Daten werden geladen...'})\n        df = pd.read_json(df_json, orient='split')\n        \n        self.update_state(state='PROGRESS', meta={'status': 'Training des XGBoost-Modells mit Default-Parametern...'})\n        result = build_and_train_xgboost(df, hparams)\n        \n        self.update_state(state='PROGRESS', meta={'status': 'Speichere Modell in GCS...'})\n        gcs = get_gcs_storage()\n        if gcs.client:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            model_path = f\"models/{ticker}/xgboost_{timestamp}.joblib\"\n            gcs.upload_joblib(result['model'], model_path)\n        \n        return {'status': 'SUCCESS', 'ticker': ticker}\n    except Exception as e:\n        logger.error(f\"XGBoost training task failed: {e}\")\n        return {'status': 'FAILED', 'error': str(e)}","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}