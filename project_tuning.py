{"cells":[{"cell_type":"code","source":"# project/tuning.py\nimport optuna\nimport logging\nfrom sklearn.model_selection import train_test_split\n\n# Import from other project modules\nfrom .training import build_and_train_lstm, build_and_train_xgboost\nfrom .features import get_feature_columns\n\nlogger = logging.getLogger(__name__)\n\ndef objective_lstm(trial, df):\n    \"\"\"Optuna objective function for LSTM hyperparameter tuning.\"\"\"\n    # Definiere den Suchraum für die Hyperparameter\n    hparams = {\n        'hidden_dim': trial.suggest_int('hidden_dim', 64, 256),\n        'n_layers': trial.suggest_int('n_layers', 1, 3),\n        'dropout_prob': trial.suggest_float('dropout_prob', 0.1, 0.5),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n        'epochs': 100, # Feste Epochenzahl, da Early Stopping greift\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n        'teacher_forcing_ratio': trial.suggest_float('teacher_forcing_ratio', 0.2, 0.8)\n    }\n    \n    try:\n        # Führe das Training mit den vorgeschlagenen Parametern durch\n        result = build_and_train_lstm(\n            df=df,\n            selected_features=get_feature_columns(),\n            hparams=hparams,\n            validation_split=0.2\n        )\n        # Optuna soll den Validierungsfehler minimieren\n        return result['val_loss']\n    except Exception as e:\n        logger.warning(f\"Trial failed with error: {e}. Pruning trial.\")\n        # Wenn ein Trial fehlschlägt (z.B. wegen ungültiger Parameter), wird er verworfen\n        raise optuna.exceptions.TrialPruned()\n\ndef objective_xgboost(trial, df):\n    \"\"\"Optuna objective function for XGBoost hyperparameter tuning.\"\"\"\n    hparams = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n        'objective': 'reg:squarederror',\n        'device': 'cuda'\n    }\n\n    try:\n        result = build_and_train_xgboost(\n            df=df,\n            hparams=hparams,\n            validation_split=0.2\n        )\n        # Da train_score der negative MSE ist, gibt ein höherer Score einen besseren Fit an.\n        # Optuna minimiert standardmäßig, daher geben wir den negativen Score zurück.\n        return -result['train_score'] # Entspricht dem MSE\n    except Exception as e:\n        logger.warning(f\"Trial failed with error: {e}. Pruning trial.\")\n        raise optuna.exceptions.TrialPruned()","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}