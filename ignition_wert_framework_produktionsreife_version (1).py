<DOCUMENT filename="ignition_wert_framework_(produktionsreife_version).py">
# -*- coding: utf-8 -*-
"""Ignition-Wert Framework (Produktionsreife Version)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/107Ftc8xBkrtZUOWfGRbXyp4KEIR3Jpz9
"""

# ==============================================================================
# 1. Installation und Importe
# ==============================================================================
!pip install flask flask-cors xgboost torch pandas numpy scikit-learn google-cloud-storage pyngrok joblib optuna flask-socketio eventlet backtrader celery redis pandas_ta

# Standardbibliotheken
import os
import json
import time
import tempfile
from datetime import datetime
import threading
import random
from functools import lru_cache

# Installierte Bibliotheken
import numpy as np
import pandas as pd
import joblib
from werkzeug.utils import secure_filename

# Machine Learning Bibliotheken
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
import pandas_ta as ta

# Webserver und Cloud
from flask import Flask, request, jsonify, abort
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from pyngrok import ngrok
from google.cloud import storage
import eventlet

# Backtesting
import backtrader as bt

# Async Tasks
from celery import Celery

# WICHTIG: eventlet.monkey_patch() ist für die asynchrone Funktionalität von SocketIO
# in Kombination mit Standardbibliotheken wie 'requests' (indirekt von GCS genutzt)
# notwendig. Es muss vor fast allen anderen Imports ausgeführt werden.
eventlet.monkey_patch()


# ==============================================================================
# 2. Konfiguration
# ==============================================================================
class Config:
    """Zentrale Konfigurationsklasse zur Vermeidung von hartcodierten Werten."""
    # GCS Konfiguration
    GCS_PROJECT_ID = "ignition-ki-csv-storage"
    GCS_BUCKET_NAME = "ignition-ki-csv-data-2025-user123"
    GCS_CREDENTIALS_PATH = 'google-credentials.json'

    # Webserver Konfiguration
    ALLOWED_CORS_ORIGINS = "*" # Für die Produktion auf eine spezifische Domain beschränken, z.B. "https://meine-frontend-app.com"

    # Modell-Hyperparameter (Standardwerte, können durch Optuna optimiert werden)
    LSTM_HPARAMS = {
        'input_dim': 9,  # 5 Basis + 4 erzeugte Features
        'hidden_dim': 128,
        'n_layers': 2,
        'dropout_prob': 0.25,
        'learning_rate': 0.001,
        'epochs': 50 # Maximale Epochen, Early Stopping wird verwendet
    }
    XGBOOST_HPARAMS = {
        'objective': 'reg:squarederror',
        'n_estimators': 150,
        'learning_rate': 0.05,
        'max_depth': 5,
        'subsample': 0.8
    }

    # Datenvorbereitung
    DATA_WINDOW_SIZE = 75
    DATA_PREDICTION_LENGTH = 30

    # Konsolidierungserkennung (Bollinger Bänder & Preiskanal)
    CONSOLIDATION_BB_WINDOW = 20
    CONSOLIDATION_LOOKBACK_PERIOD = 50
    CONSOLIDATION_CHANNEL_THRESHOLD = 0.05 # Max. 5% der Kurse dürfen außerhalb der Box sein

    # Modell-Registrierung
    MODEL_REGISTRY_PATH = 'model_registry.json'

    # Backtesting
    BACKTEST_INITIAL_CASH = 100000
    BACKTEST_COMMISSION = 0.001

    # API
    MAX_FILE_SIZE = 1024 * 1024  # 1MB

    # Celery
    CELERY_BROKER_URL = 'redis://localhost:6379/0'
    CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'

    # Signal
    ATR_MULTIPLIER = 3  # Für dynamischen Schwellenwert
    ADX_THRESHOLD = 25  # Für Trendstärke (niedrig = keine Trend)


# ==============================================================================
# 3. GCS-Integration
# ==============================================================================
class GCSStorage:
    """Verwaltet die Speicherung und das Laden von Daten in Google Cloud Storage."""
    def __init__(self, project_id, bucket_name, credentials_path):
        if os.path.exists(credentials_path):
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
            try:
                self.client = storage.Client(project=project_id)
                self.bucket = self.client.bucket(bucket_name)
                if not self.bucket.exists():
                    self.bucket.create()
            except Exception as e:
                self.bucket = None
        else:
            self.bucket = None

    def save_ticker_data(self, ticker, data_dict):
        """Speichert die Ticker-Daten (aus einem DataFrame) als JSON in GCS."""
        if not self.bucket:
            return False
        try:
            file_data = {'ticker': ticker, 'data': data_dict, 'timestamp': datetime.now().isoformat(), 'data_points': len(data_dict)}
            blob_name = f"data/{ticker}/{datetime.now().strftime('%Y-%m-%d')}.json"
            blob = self.bucket.blob(blob_name)
            blob.upload_from_string(json.dumps(file_data, indent=2), content_type='application/json')
            return True
        except Exception as e:
            return False

# Initialisiere GCS-Instanz
gcs_storage = GCSStorage(Config.GCS_PROJECT_ID, Config.GCS_BUCKET_NAME, Config.GCS_CREDENTIALS_PATH)


# ==============================================================================
# 4. Webserver-Initialisierung
# ==============================================================================
app = Flask(__name__)
app.config['CELERY_BROKER_URL'] = Config.CELERY_BROKER_URL
app.config['CELERY_RESULT_BACKEND'] = Config.CELERY_RESULT_BACKEND
CORS(app, resources={r"/*": {"origins": Config.ALLOWED_CORS_ORIGINS}})
socketio = SocketIO(app, async_mode='eventlet')
celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL'])
celery.conf.update(app.config)


# ==============================================================================
# 5. LSTM Seq2Seq Modellarchitektur
# ==============================================================================
class Encoder(nn.Module):
    """Der Encoder-Teil des Seq2Seq-Modells."""
    def __init__(self, input_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, src):
        outputs, (hidden, cell) = self.rnn(src)
        hidden = hidden.view(self.n_layers, 2, -1, self.hidden_dim)
        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)
        hidden = self.dropout(hidden)
        cell = cell.view(self.n_layers, 2, -1, self.hidden_dim)
        cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)
        cell = self.dropout(cell)
        return hidden, cell

class Decoder(nn.Module):
    """Der Decoder-Teil des Seq2Seq-Modells."""
    def __init__(self, output_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.rnn = nn.LSTM(output_dim, hidden_dim * 2, n_layers, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(1)
        output, (hidden, cell) = self.rnn(input, (hidden, cell))
        prediction = self.fc_out(self.dropout(output.squeeze(1)))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    """Das gesamte Seq2Seq-Modell, das Encoder und Decoder kombiniert."""
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len, trg_dim = src.shape[0], trg.shape[1], trg.shape[2]
        outputs = torch.zeros(batch_size, trg_len, trg_dim).to(self.device)
        hidden, cell = self.encoder(src)
        input = trg[:, 0, :]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t, :] = output
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[:, t, :] if teacher_force else output
        return outputs

    def predict(self, src, prediction_length=30):
        self.eval()
        with torch.no_grad():
            batch_size, trg_dim = src.shape[0], src.shape[2]
            outputs = torch.zeros(batch_size, prediction_length, trg_dim).to(self.device)
            hidden, cell = self.encoder(src)
            input = src[:, -1, :]
            for t in range(prediction_length):
                output, hidden, cell = self.decoder(input, hidden, cell)
                outputs[:, t, :] = output
                input = output
        return outputs


# ==============================================================================
# 6. Hilfsfunktionen (Loss, Datenvorbereitung, Signallogik)
# ==============================================================================
class AsymmetricWeightedMSELoss(nn.Module):
    """Benutzerdefinierte Loss-Funktion, die katastrophale Fehler stärker bestraft."""
    def __init__(self, catastrophe_penalty=10.0, correct_breakout_bonus=2.0):
        super().__init__()
        self.catastrophe_penalty = catastrophe_penalty
        self.correct_breakout_bonus = correct_breakout_bonus

    def forward(self, predictions, targets, upper_bound, lower_bound):
        base_mse = (predictions - targets) ** 2
        weights = torch.ones_like(base_mse)
        catastrophe_condition = (predictions > upper_bound) & (targets < lower_bound)
        weights[catastrophe_condition] = self.catastrophe_penalty
        correct_breakout_condition = (predictions > upper_bound) & (targets > upper_bound)
        weights[correct_breakout_condition] = self.correct_breakout_bonus
        weighted_mse = weights * base_mse
        return torch.mean(weighted_mse)

def _create_features(df):
    """
    Erzeugt konsistent technische Indikatoren als Features.
    VERBESSERT: Fügt RSI, MACD, ATR und ADX hinzu.
    """
    df_copy = df.copy()
    df_copy['range'] = df_copy['high'] - df_copy['low']
    df_copy['avg_price'] = (df_copy['high'] + df_copy['low']) / 2

    delta = df_copy['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df_copy['rsi'] = 100 - (100 / (1 + rs))

    exp1 = df_copy['close'].ewm(span=12, adjust=False).mean()
    exp2 = df_copy['close'].ewm(span=26, adjust=False).mean()
    df_copy['macd'] = exp1 - exp2

    # ATR und ADX hinzufügen
    df_copy.ta.atr(append=True)
    df_copy.ta.adx(append=True)

    df_copy.fillna(method='bfill', inplace=True)
    df_copy.fillna(method='ffill', inplace=True)
    return df_copy

def prepare_sequences(df, window_size, prediction_length, step=1, scaler=None):
    """Bereitet Trainingsdaten mit einem Sliding-Window-Ansatz aus einem DataFrame vor."""
    if scaler is None:
        scaler = MinMaxScaler()

    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']

    df_with_features = _create_features(df)
    if scaler.feature_range == (0, 1):  # Assuming it's fitted or not
        data = scaler.fit_transform(df_with_features[feature_cols]) if scaler.steps is None else scaler.transform(df_with_features[feature_cols])

    src_sequences, trg_sequences = [], []
    for i in range(0, len(data) - window_size - prediction_length + 1, step):
        src_sequences.append(data[i:i + window_size])
        trg_sequences.append(data[i + window_size:i + window_size + prediction_length])

    return np.array(src_sequences), np.array(trg_sequences), scaler

def prepare_inference_from_df(df, scaler, window_size):
    """Bereitet die letzte Sequenz aus einem DataFrame für die Vorhersage vor."""
    df_with_features = _create_features(df)
    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']
    data = scaler.transform(df_with_features[feature_cols])

    if len(data) < window_size:
        return np.array([])

    src_sequence = data[-window_size:]
    return np.array([src_sequence])

def identify_consolidation(df, price_col='close', window=20, lookback_period=50, channel_threshold=0.05):
    """
    VERBESSERT: Identifiziert eine Konsolidierung basierend auf einem Zwei-Faktoren-Modell plus ADX.
    """
    if len(df) < lookback_period:
        return False, None, None

    middle_band = df[price_col].rolling(window).mean()
    std_dev = df[price_col].rolling(window).std()
    upper_band = middle_band + (2 * std_dev)
    lower_band = middle_band - (2 * std_dev)
    bandwidth = (upper_band - lower_band) / middle_band

    squeeze_threshold = bandwidth.rolling(lookback_period).quantile(0.10).iloc[-1]
    is_in_squeeze = bandwidth.iloc[-1] < squeeze_threshold

    if not is_in_squeeze:
        return False, None, None

    recent_data = df.iloc[-lookback_period:]
    consolidation_high = recent_data['high'].max()
    consolidation_low = recent_data['low'].min()

    outliers = recent_data[(recent_data[price_col] > consolidation_high) | (recent_data[price_col] < consolidation_low)]

    is_channel_valid = (len(outliers) / lookback_period) <= channel_threshold

    # Zusätzlich ADX prüfen (niedriger ADX = schwacher Trend)
    adx = recent_data['ADX_14'].iloc[-1] if 'ADX_14' in recent_data.columns else None
    is_low_trend = adx < Config.ADX_THRESHOLD if adx is not None else True

    if is_in_squeeze and is_channel_valid and is_low_trend:
        return True, consolidation_low, consolidation_high

    return False, None, None


# ==============================================================================
# 7. Modell-Trainingsfunktionen (Async mit Celery)
# ==============================================================================
@celery.task
def train_lstm_async(df, hparams, ticker):
    """
    Asynchrone Trainingsfunktion für LSTM.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_df, val_df = train_test_split(df, test_size=0.2, shuffle=False)

    src, trg, scaler = prepare_sequences(train_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH)
    val_src, val_trg, _ = prepare_sequences(val_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH, scaler=scaler)  # Use train scaler

    if src.shape[0] == 0 or val_src.shape[0] == 0:
        return None, None, float('inf')

    input_dim = src.shape[-1]

    encoder = Encoder(input_dim, hparams['hidden_dim'], hparams['n_layers'], hparams['dropout_prob']).to(device)
    decoder = Decoder(input_dim, hparams['hidden_dim'], hparams['n_layers'], hparams['dropout_prob']).to(device)
    model = Seq2Seq(encoder, decoder, device).to(device)

    # Quantisierung für schnellere Inferenz
    model = torch.quantization.quantize_dynamic(model, {nn.LSTM}, dtype=torch.qint8)

    optimizer = optim.Adam(model.parameters(), lr=hparams['learning_rate'])
    train_criterion = AsymmetricWeightedMSELoss()
    val_criterion = nn.MSELoss()

    src_tensor = torch.tensor(src, dtype=torch.float32).to(device)
    trg_tensor = torch.tensor(trg, dtype=torch.float32).to(device)

    last_known_close = src_tensor[:, -1, 3]
    historical_volatility = src_tensor[:, :, 3].std(dim=1)
    upper_bound_val = last_known_close + (2 * historical_volatility)
    lower_bound_val = last_known_close - (2 * historical_volatility)
    upper_bound = upper_bound_val.unsqueeze(1).unsqueeze(2).expand_as(trg_tensor)
    lower_bound = lower_bound_val.unsqueeze(1).unsqueeze(2).expand_as(trg_tensor)

    dataset = TensorDataset(src_tensor, trg_tensor, upper_bound, lower_bound)
    loader = DataLoader(dataset, batch_size=16, shuffle=True)

    val_src_tensor = torch.tensor(val_src, dtype=torch.float32).to(device)
    val_trg_tensor = torch.tensor(val_trg, dtype=torch.float32).to(device)

    best_val_loss = float('inf')
    patience_counter = 0
    patience = 5
    best_model_path, best_scaler_path = None, None

    for epoch in range(hparams['epochs']):
        model.train()
        epoch_loss = 0
        for src_batch, trg_batch, upper_batch, lower_batch in loader:
            optimizer.zero_grad()
            output = model(src_batch, trg_batch)
            loss = train_criterion(output[:, 1:, :], trg_batch[:,1:, :], upper_batch[:, 1:, :], lower_batch[:,1:, :])
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_output = model(val_src_tensor, val_trg_tensor)
            val_loss = val_criterion(val_output[:, 1:, :], val_trg_tensor[:,1:, :])

        avg_loss = epoch_loss / len(loader)
        socketio.emit('training_progress', {'model': 'LSTM', 'epoch': epoch + 1, 'loss': avg_loss, 'val_loss': val_loss.item(), 'ticker': ticker})

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            model_version = datetime.now().strftime("%Y%m%d%H%M%S")
            best_model_path = f'lstm_model_{ticker}_v{model_version}.pth'
            best_scaler_path = f'lstm_scaler_{ticker}_v{model_version}.pkl'
            torch.save(model.state_dict(), best_model_path)
            joblib.dump(scaler, best_scaler_path)
        else:
            patience_counter += 1

        if patience_counter >= patience:
            break

    return best_model_path, best_scaler_path, best_val_loss.item()


@celery.task
def train_xgboost_async(df, hparams, ticker):
    """Asynchrone Trainingsfunktion für XGBoost."""
    df_with_features = _create_features(df)
    feature_cols = ['open', 'high', 'low', 'close', 'avg_price', 'volume', 'range', 'rsi', 'macd']
    features = df_with_features[feature_cols].values
    targets = df_with_features['close'].values

    if len(features) < 20:
        return None, float('inf')

    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)
    model = xgb.XGBRegressor(**hparams)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)

    val_score = model.best_score

    model_version = datetime.now().strftime("%Y%m%d%H%M%S")
    model_path = f'xgboost_model_{ticker}_v{model_version}.pkl'
    joblib.dump(model, model_path)
    return model_path, val_score

# ==============================================================================
# 8. Hyperparameter-Tuning mit Optuna (Async)
# ==============================================================================
@celery.task
def tune_lstm_hyperparameters_async(df, ticker):
    """Asynchrone Tuning für LSTM."""
    train_df, val_df = train_test_split(df, test_size=0.2, shuffle=False)
    src, trg, scaler = prepare_sequences(train_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH)
    val_src, val_trg, _ = prepare_sequences(val_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH, scaler=scaler)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def objective(trial):
        params = {
            'hidden_dim': trial.suggest_int('hidden_dim', 64, 256),
            'n_layers': trial.suggest_int('n_layers', 1, 3),
            'dropout_prob': trial.suggest_float('dropout_prob', 0.1, 0.3),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'epochs': 20
        }

        encoder = Encoder(src.shape[-1], params['hidden_dim'], params['n_layers'], params['dropout_prob']).to(device)
        decoder = Decoder(src.shape[-1], params['hidden_dim'], params['n_layers'], params['dropout_prob']).to(device)
        model = Seq2Seq(encoder, decoder, device).to(device)

        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
        criterion = nn.MSELoss()

        src_tensor = torch.tensor(src, dtype=torch.float32).to(device)
        trg_tensor = torch.tensor(trg, dtype=torch.float32).to(device)

        val_src_tensor = torch.tensor(val_src, dtype=torch.float32).to(device)
        val_trg_tensor = torch.tensor(val_trg, dtype=torch.float32).to(device)

        for epoch in range(params['epochs']):
            model.train()
            output = model(src_tensor, trg_tensor)
            loss = criterion(output[:, 1:, :], trg_tensor[:, 1:, :])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad():
            val_output = model(val_src_tensor, val_trg_tensor)
            val_loss = criterion(val_output, val_trg_tensor)

        return val_loss.item()

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=20)
    return study.best_params

@celery.task
def tune_xgboost_hyperparameters_async(df, ticker):
    """Asynchrone Tuning für XGBoost."""
    df_with_features = _create_features(df)
    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'avg_price', 'rsi', 'macd']
    X = df_with_features[feature_cols].values
    y = df_with_features['close'].values
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    def objective(trial):
        params = {
            'objective': 'reg:squarederror',
            'n_estimators': trial.suggest_int('n_estimators', 50, 500),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 8),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        }
        model = xgb.XGBRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)
        return model.best_score

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=10)
    return study.best_params

# ==============================================================================
# 9. Backtesting-Funktion
# ==============================================================================
class BreakoutStrategy(bt.Strategy):
    def __init__(self, precomputed_df, trained_lstm, lstm_scaler, trained_xgboost):
        self.precomputed_df = precomputed_df  # Precomputed features
        self.trained_lstm = trained_lstm
        self.lstm_scaler = lstm_scaler
        self.trained_xgboost = trained_xgboost
        self.bar_num = 0

    def next(self):
        if self.bar_num >= len(self.precomputed_df) - Config.DATA_WINDOW_SIZE:
            return

        current_df = self.precomputed_df.iloc[:self.bar_num + Config.DATA_WINDOW_SIZE]
        sequences = prepare_inference_from_df(current_df, self.lstm_scaler, Config.DATA_WINDOW_SIZE)
        if sequences.size == 0:
            self.bar_num += 1
            return

        src_tensor = torch.tensor(sequences, dtype=torch.float32).to(self.trained_lstm.device)
        lstm_preds = self.trained_lstm.predict(src_tensor, 1).squeeze(0).squeeze(0).cpu().numpy()
        features = current_df[feature_cols].values[-1:]  # Use precomputed
        xgboost_pred = self.trained_xgboost.predict(features)[0]

        current_close = current_df['close'].iloc[-1]
        signal, _, _ = calculate_breakout_signal(lstm_preds, xgboost_pred, current_close, current_df)

        if signal == "STRONG_BULLISH_BREAKOUT" and not self.position:
            self.buy()
        elif signal == "STRONG_BEARISH_BREAKOUT" and self.position:
            self.sell()

        self.bar_num += 1

def backtest_models(df, trained_lstm, lstm_scaler, trained_xgboost):
    """Führt Backtesting durch und berechnet Performance-Metrics."""
    precomputed_df = _create_features(df)  # Precompute once

    cerebro = bt.Cerebro()
    cerebro.addstrategy(BreakoutStrategy, precomputed_df=precomputed_df, trained_lstm=trained_lstm, lstm_scaler=lstm_scaler, trained_xgboost=trained_xgboost)
    data = bt.feeds.PandasData(dataname=df.set_index('date'))
    cerebro.adddata(data)
    cerebro.broker.setcash(Config.BACKTEST_INITIAL_CASH)
    cerebro.broker.setcommission(commission=Config.BACKTEST_COMMISSION)
    cerebro.run()

    final_portfolio = cerebro.broker.getvalue()
    returns = (final_portfolio - Config.BACKTEST_INITIAL_CASH) / Config.BACKTEST_INITIAL_CASH * 100
    daily_returns = df['close'].pct_change().dropna().values
    sharpe = np.mean(daily_returns) / np.std(daily_returns) * np.sqrt(252)

    return {
        'returns_percent': returns,
        'sharpe_ratio': sharpe
    }

# ==============================================================================
# 10. Modell-Laden mit Caching
# ==============================================================================
@lru_cache(maxsize=10)
def load_lstm_model(ticker):
    # Annahme: Modelle werden pro Ticker gespeichert
    model_path = f'lstm_model_{ticker}.pth'  # Vereinfacht
    scaler_path = f'lstm_scaler_{ticker}.pkl'
    device = torch.device('cpu')
    encoder = Encoder(Config.LSTM_HPARAMS['input_dim'], Config.LSTM_HPARAMS['hidden_dim'], Config.LSTM_HPARAMS['n_layers'], Config.LSTM_HPARAMS['dropout_prob']).to(device)
    decoder = Decoder(Config.LSTM_HPARAMS['input_dim'], Config.LSTM_HPARAMS['hidden_dim'], Config.LSTM_HPARAMS['n_layers'], Config.LSTM_HPARAMS['dropout_prob']).to(device)
    model = Seq2Seq(encoder, decoder, device).to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    scaler = joblib.load(scaler_path)
    return model, scaler

@lru_cache(maxsize=10)
def load_xgboost_model(ticker):
    model_path = f'xgboost_model_{ticker}.pkl'
    return joblib.load(model_path)

# ==============================================================================
# 11. Flask API Routen
# ==============================================================================
def calculate_breakout_signal(lstm_preds, ignition_value, current_close, df):
    is_consolidating, low, high = identify_consolidation(
        df,
        'close',
        Config.CONSOLIDATION_BB_WINDOW,
        Config.CONSOLIDATION_LOOKBACK_PERIOD,
        Config.CONSOLIDATION_CHANNEL_THRESHOLD
    )

    projected_close = lstm_preds[-1][3]  # Index 3 ist 'close'
    projected_gain = (projected_close - current_close) / current_close * 100 if current_close > 0 else 0

    # Dynamischer Schwellenwert basierend auf ATR
    atr = df['ATRr_14'].iloc[-1] if 'ATRr_14' in df.columns else 0
    threshold = Config.ATR_MULTIPLIER * atr / current_close * 100 if current_close > 0 else 30

    signal = "NO_SIGNAL"
    if is_consolidating:
        if ignition_value > current_close and projected_gain >= threshold:
            signal = "STRONG_BULLISH_BREAKOUT"
        elif ignition_value < current_close and projected_gain <= -threshold:
            signal = "STRONG_BEARISH_BREAKOUT"
        else:
            signal = "CONSOLIDATING_NO_BREAKOUT"

    return signal, projected_gain, (low, high) if is_consolidating else (None, None)

@app.route('/predict', methods=['POST'])
def predict_breakout():
    if 'file' not in request.files:
        return jsonify({'error': 'Keine Datei im Request gefunden'}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Keine Datei ausgewählt'}), 400

    # Dateigrößenprüfung
    if len(file.read()) > Config.MAX_FILE_SIZE:
        return jsonify({'error': 'Datei zu groß'}), 413
    file.seek(0)  # Reset stream

    if file and file.filename.endswith('.csv'):
        try:
            df = pd.read_csv(file)
        except Exception as e:
            return jsonify({'error': f'Fehler beim Lesen der CSV-Datei: {str(e)}'}), 400

        required_columns = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_columns):
            return jsonify({'error': f'CSV fehlen benötigte Spalten: {required_columns}'}), 400

        ticker = file.filename.split('.')[0]
        gcs_storage.save_ticker_data(ticker, df.to_dict(orient='records'))

        # Starte async Training/Tuning, falls nötig (z.B. bei neuem Ticker)
        tune_lstm_hyperparameters_async.delay(df, ticker)
        train_lstm_async.delay(df, Config.LSTM_HPARAMS, ticker)

        # Lade gecachte Modelle
        trained_lstm, lstm_scaler = load_lstm_model(ticker)
        trained_xgboost = load_xgboost_model(ticker)

        try:
            sequences = prepare_inference_from_df(df, lstm_scaler, Config.DATA_WINDOW_SIZE)
        except ValueError as e:
            return jsonify({'error': f'Fehler bei der Datenvorbereitung: {e}'}), 400

        if sequences.size == 0:
            return jsonify({'error': 'Nicht genügend Daten in der Datei für eine Vorhersage.'}), 400

        src_tensor_pred = torch.tensor(sequences, dtype=torch.float32).to(trained_lstm.device)

        with torch.no_grad():
            lstm_preds = trained_lstm.predict(src_tensor_pred, Config.DATA_PREDICTION_LENGTH)

        unscaled_lstm_preds = lstm_scaler.inverse_transform(lstm_preds.squeeze(0).cpu().numpy())

        df_with_features = _create_features(df.copy())
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']
        features = df_with_features[feature_cols].values[-1:]
        xgboost_pred = trained_xgboost.predict(features)[0]

        current_close = df['close'].iloc[-1]
        signal, projected_gain, consolidation_channel = calculate_breakout_signal(unscaled_lstm_preds, xgboost_pred, current_close, df_with_features)

        response_data = {
            'lstm_prediction_30_steps': unscaled_lstm_preds.tolist(),
            'xgboost_ignition_value': float(xgboost_pred),
            'signal': signal,
            'projected_gain_percent': projected_gain,
            'current_close': current_close,
            'consolidation_channel': {'low': consolidation_channel[0], 'high': consolidation_channel[1]} if consolidation_channel[0] is not None else None
        }
        return jsonify(response_data)

    return jsonify({'error': 'Ungültiger Dateityp, bitte eine .csv-Datei hochladen'}), 400

@app.route('/backtest', methods=['POST'])
def run_backtest():
    if 'file' not in request.files:
        return jsonify({'error': 'Keine Datei im Request gefunden'}), 400

    file = request.files['file']
    if len(file.read()) > Config.MAX_FILE_SIZE:
        return jsonify({'error': 'Datei zu groß'}), 413
    file.seek(0)

    if file.filename.endswith('.csv'):
        df = pd.read_csv(file)
        df['date'] = pd.to_datetime(df['date'])
        # Annahme: Modelle geladen
        trained_lstm, lstm_scaler = load_lstm_model('default')  # Oder spezifisch
        trained_xgboost = load_xgboost_model('default')
        results = backtest_models(df, trained_lstm, lstm_scaler, trained_xgboost)
        return jsonify(results)
    return jsonify({'error': 'Ungültiger Dateityp'}), 400

@app.route('/')
def index():
    return "LSTM & XGBoost Prediction Server is running!"

# ==============================================================================
# 12. Server Start
# ==============================================================================
try:
    ngrok.kill()
except:
    pass

public_url = ngrok.connect(5000)
print(f" * Public URL: {public_url}")

if __name__ == '__main__':
    def run_app():
        socketio.run(app, host='0.0.0.0', port=5000)

    flask_thread = threading.Thread(target=run_app)
    flask_thread.daemon = True
    flask_thread.start()
</DOCUMENT>