# -*- coding: utf-8 -*-
"""Ignition-Wert Framework (Produktionsreife Version)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/107Ftc8xBkrtZUOWfGRbXyp4KEIR3Jpz9
"""

# ==============================================================================
# 1. Installation und Importe
# ==============================================================================
# Die Option -q wurde entfernt, um Installationsdetails sichtbar zu machen.
!pip install flask flask-cors xgboost torch pandas numpy scikit-learn google-cloud-storage pyngrok joblib optuna flask-socketio eventlet

# Standardbibliotheken
import os
import json
import time
import tempfile
from datetime import datetime
import threading

# Installierte Bibliotheken
import numpy as np
import pandas as pd
import joblib
from werkzeug.utils import secure_filename

# Machine Learning Bibliotheken
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna

# Webserver und Cloud
from flask import Flask, request, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from pyngrok import ngrok
from google.cloud import storage
import eventlet

# WICHTIG: eventlet.monkey_patch() ist für die asynchrone Funktionalität von SocketIO
# in Kombination mit Standardbibliotheken wie 'requests' (indirekt von GCS genutzt)
# notwendig. Es muss vor fast allen anderen Imports ausgeführt werden.
eventlet.monkey_patch()


# ==============================================================================
# 2. Konfiguration
# ==============================================================================
class Config:
    """Zentrale Konfigurationsklasse zur Vermeidung von hartcodierten Werten."""
    # GCS Konfiguration
    GCS_PROJECT_ID = "ignition-ki-csv-storage"
    GCS_BUCKET_NAME = "ignition-ki-csv-data-2025-user123"
    GCS_CREDENTIALS_PATH = 'google-credentials.json'

    # Webserver Konfiguration
    ALLOWED_CORS_ORIGINS = "*" # Für die Produktion auf eine spezifische Domain beschränken, z.B. "https://meine-frontend-app.com"

    # Modell-Hyperparameter (Standardwerte, können durch Optuna optimiert werden)
    LSTM_HPARAMS = {
        'input_dim': 9,  # 5 Basis + 4 erzeugte Features
        'hidden_dim': 128,
        'n_layers': 2,
        'dropout_prob': 0.25,
        'learning_rate': 0.001,
        'epochs': 50 # Maximale Epochen, Early Stopping wird verwendet
    }
    XGBOOST_HPARAMS = {
        'objective': 'reg:squarederror',
        'n_estimators': 150,
        'learning_rate': 0.05,
        'max_depth': 5,
        'subsample': 0.8
    }

    # Datenvorbereitung
    DATA_WINDOW_SIZE = 75
    DATA_PREDICTION_LENGTH = 30

    # Konsolidierungserkennung (Bollinger Bänder & Preiskanal)
    CONSOLIDATION_BB_WINDOW = 20
    CONSOLIDATION_LOOKBACK_PERIOD = 50
    CONSOLIDATION_CHANNEL_THRESHOLD = 0.05 # Max. 5% der Kurse dürfen außerhalb der Box sein

    # Modell-Registrierung
    MODEL_REGISTRY_PATH = 'model_registry.json'


# ==============================================================================
# 3. GCS-Integration
# ==============================================================================
class GCSStorage:
    """Verwaltet die Speicherung und das Laden von Daten in Google Cloud Storage."""
    def __init__(self, project_id, bucket_name, credentials_path):
        if os.path.exists(credentials_path):
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
            try:
                self.client = storage.Client(project=project_id)
                self.bucket = self.client.bucket(bucket_name)
                if not self.bucket.exists():
                    print(f"⚠️ Bucket {bucket_name} existiert nicht. Er wird erstellt.")
                    self.bucket.create()
                print(f"✅ GCS initialisiert: {bucket_name}")
            except Exception as e:
                print(f"❌ GCS Fehler: {str(e)}. Fallback zu lokalem Speicher.")
                self.bucket = None
        else:
            print("⚠️ Google Credentials nicht gefunden. GCS ist deaktiviert.")
            self.bucket = None

    def save_ticker_data(self, ticker, data_dict):
        """Speichert die Ticker-Daten (aus einem DataFrame) als JSON in GCS."""
        if not self.bucket:
            return False
        try:
            file_data = {'ticker': ticker, 'data': data_dict, 'timestamp': datetime.now().isoformat(), 'data_points': len(data_dict)}
            blob_name = f"data/{ticker}/{datetime.now().strftime('%Y-%m-%d')}.json"
            blob = self.bucket.blob(blob_name)
            blob.upload_from_string(json.dumps(file_data, indent=2), content_type='application/json')
            print(f"✅ Ticker-Daten '{ticker}' nach GCS hochgeladen.")
            return True
        except Exception as e:
            print(f"❌ Fehler beim Hochladen der Ticker-Daten: {e}")
            return False

# Initialisiere GCS-Instanz
gcs_storage = GCSStorage(Config.GCS_PROJECT_ID, Config.GCS_BUCKET_NAME, Config.GCS_CREDENTIALS_PATH)


# ==============================================================================
# 4. Webserver-Initialisierung
# ==============================================================================
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": Config.ALLOWED_CORS_ORIGINS}})
socketio = SocketIO(app, async_mode='eventlet')


# ==============================================================================
# 5. LSTM Seq2Seq Modellarchitektur
# ==============================================================================
class Encoder(nn.Module):
    """Der Encoder-Teil des Seq2Seq-Modells."""
    def __init__(self, input_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, src):
        outputs, (hidden, cell) = self.rnn(src)
        hidden = hidden.view(self.n_layers, 2, -1, self.hidden_dim)
        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)
        hidden = self.dropout(hidden)
        cell = cell.view(self.n_layers, 2, -1, self.hidden_dim)
        cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)
        cell = self.dropout(cell)
        return hidden, cell

class Decoder(nn.Module):
    """Der Decoder-Teil des Seq2Seq-Modells."""
    def __init__(self, output_dim, hidden_dim, n_layers, dropout_prob):
        super().__init__()
        self.rnn = nn.LSTM(output_dim, hidden_dim * 2, n_layers, dropout=dropout_prob if n_layers > 1 else 0, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(1)
        output, (hidden, cell) = self.rnn(input, (hidden, cell))
        prediction = self.fc_out(self.dropout(output.squeeze(1)))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    """Das gesamte Seq2Seq-Modell, das Encoder und Decoder kombiniert."""
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len, trg_dim = src.shape[0], trg.shape[1], trg.shape[2]
        outputs = torch.zeros(batch_size, trg_len, trg_dim).to(self.device)
        hidden, cell = self.encoder(src)
        input = trg[:, 0, :]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t, :] = output
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[:, t, :] if teacher_force else output
        return outputs

    def predict(self, src, prediction_length=30):
        self.eval()
        with torch.no_grad():
            batch_size, trg_dim = src.shape[0], src.shape[2]
            outputs = torch.zeros(batch_size, prediction_length, trg_dim).to(self.device)
            hidden, cell = self.encoder(src)
            input = src[:, -1, :]
            for t in range(prediction_length):
                output, hidden, cell = self.decoder(input, hidden, cell)
                outputs[:, t, :] = output
                input = output
        return outputs

# ==============================================================================
# 6. Hilfsfunktionen (Loss, Datenvorbereitung, Signallogik)
# ==============================================================================
class AsymmetricWeightedMSELoss(nn.Module):
    """Benutzerdefinierte Loss-Funktion, die katastrophale Fehler stärker bestraft."""
    def __init__(self, catastrophe_penalty=10.0, correct_breakout_bonus=2.0):
        super().__init__()
        self.catastrophe_penalty = catastrophe_penalty
        self.correct_breakout_bonus = correct_breakout_bonus

    def forward(self, predictions, targets, upper_bound, lower_bound):
        base_mse = (predictions - targets) ** 2
        weights = torch.ones_like(base_mse)
        catastrophe_condition = (predictions > upper_bound) & (targets < lower_bound)
        weights[catastrophe_condition] = self.catastrophe_penalty
        correct_breakout_condition = (predictions > upper_bound) & (targets > upper_bound)
        weights[correct_breakout_condition] = self.correct_breakout_bonus
        weighted_mse = weights * base_mse
        return torch.mean(weighted_mse)

def _create_features(df):
    """
    Erzeugt konsistent technische Indikatoren als Features.
    VERBESSERT: Fügt RSI und MACD hinzu.
    """
    df_copy = df.copy()
    df_copy['range'] = df_copy['high'] - df_copy['low']
    df_copy['avg_price'] = (df_copy['high'] + df_copy['low']) / 2

    delta = df_copy['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df_copy['rsi'] = 100 - (100 / (1 + rs))

    exp1 = df_copy['close'].ewm(span=12, adjust=False).mean()
    exp2 = df_copy['close'].ewm(span=26, adjust=False).mean()
    df_copy['macd'] = exp1 - exp2

    df_copy.fillna(method='bfill', inplace=True)
    df_copy.fillna(method='ffill', inplace=True)
    return df_copy

def prepare_sequences(df, window_size, prediction_length, step=1):
    """Bereitet Trainingsdaten mit einem Sliding-Window-Ansatz aus einem DataFrame vor."""
    scaler = MinMaxScaler()
    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']

    df_with_features = _create_features(df)
    data = scaler.fit_transform(df_with_features[feature_cols])

    src_sequences, trg_sequences = [], []
    for i in range(0, len(data) - window_size - prediction_length + 1, step):
        src_sequences.append(data[i:i + window_size])
        trg_sequences.append(data[i + window_size:i + window_size + prediction_length])

    return np.array(src_sequences), np.array(trg_sequences), scaler

def prepare_inference_from_df(df, scaler, window_size):
    """Bereitet die letzte Sequenz aus einem DataFrame für die Vorhersage vor."""
    df_with_features = _create_features(df)
    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']
    data = scaler.transform(df_with_features[feature_cols])

    if len(data) < window_size:
        return np.array([])

    src_sequence = data[-window_size:]
    return np.array([src_sequence])

def identify_consolidation(df, price_col='close', window=20, lookback_period=50, channel_threshold=0.05):
    """
    VERBESSERT: Identifiziert eine Konsolidierung basierend auf einem Zwei-Faktoren-Modell.
    """
    if len(df) < lookback_period:
        return False, None, None

    middle_band = df[price_col].rolling(window).mean()
    std_dev = df[price_col].rolling(window).std()
    upper_band = middle_band + (2 * std_dev)
    lower_band = middle_band - (2 * std_dev)
    bandwidth = (upper_band - lower_band) / middle_band

    squeeze_threshold = bandwidth.rolling(lookback_period).quantile(0.10).iloc[-1]
    is_in_squeeze = bandwidth.iloc[-1] < squeeze_threshold

    if not is_in_squeeze:
        return False, None, None

    recent_data = df.iloc[-lookback_period:]
    consolidation_high = recent_data['high'].max()
    consolidation_low = recent_data['low'].min()

    outliers = recent_data[(recent_data[price_col] > consolidation_high) | (recent_data[price_col] < consolidation_low)]

    is_channel_valid = (len(outliers) / lookback_period) <= channel_threshold

    if is_in_squeeze and is_channel_valid:
        return True, consolidation_low, consolidation_high

    return False, None, None

# ==============================================================================
# 5. Modell-Trainingsfunktionen
# ==============================================================================
def train_lstm(df, hparams):
    """
    Trainiert das LSTM-Modell, inklusive Validierung und Early Stopping.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"LSTM Training auf: {device}")

    train_df, val_df = train_test_split(df, test_size=0.2, shuffle=False)

    src, trg, scaler = prepare_sequences(train_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH)
    # Wichtig: Den Scaler vom Trainingsdatensatz für die Validierungsdaten verwenden
    val_src, val_trg, _ = prepare_sequences(val_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH)


    if src.shape[0] == 0 or val_src.shape[0] == 0:
        print("Nicht genügend Daten für Training und Validierung.")
        return None, None, float('inf')

    input_dim = src.shape[-1]

    encoder = Encoder(input_dim, hparams['hidden_dim'], hparams['n_layers'], hparams['dropout_prob']).to(device)
    decoder = Decoder(input_dim, hparams['hidden_dim'], hparams['n_layers'], hparams['dropout_prob']).to(device)
    model = Seq2Seq(encoder, decoder, device).to(device)

    optimizer = optim.Adam(model.parameters(), lr=hparams['learning_rate'])
    # KORRIGIERT: Asymmetrische Loss für Training, Standard-MSE für Validierung
    train_criterion = AsymmetricWeightedMSELoss()
    val_criterion = nn.MSELoss()

    src_tensor = torch.tensor(src, dtype=torch.float32).to(device)
    trg_tensor = torch.tensor(trg, dtype=torch.float32).to(device)

    last_known_close = src_tensor[:, -1, 3]
    historical_volatility = src_tensor[:, :, 3].std(dim=1)
    upper_bound_val = last_known_close + (2 * historical_volatility)
    lower_bound_val = last_known_close - (2 * historical_volatility)
    upper_bound = upper_bound_val.unsqueeze(1).unsqueeze(2).expand_as(trg_tensor)
    lower_bound = lower_bound_val.unsqueeze(1).unsqueeze(2).expand_as(trg_tensor)

    dataset = TensorDataset(src_tensor, trg_tensor, upper_bound, lower_bound)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    val_src_tensor = torch.tensor(val_src, dtype=torch.float32).to(device)
    val_trg_tensor = torch.tensor(val_trg, dtype=torch.float32).to(device)

    best_val_loss = float('inf')
    patience_counter = 0
    patience = 5
    best_model_path, best_scaler_path = None, None

    for epoch in range(hparams['epochs']):
        model.train()
        epoch_loss = 0
        for src_batch, trg_batch, upper_batch, lower_batch in loader:
            optimizer.zero_grad()
            output = model(src_batch, trg_batch)
            loss = train_criterion(output[:, 1:, :], trg_batch[:, 1:, :], upper_batch[:, 1:, :], lower_batch[:, 1:, :])
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_output = model(val_src_tensor, val_trg_tensor)
            val_loss = val_criterion(val_output[:, 1:, :], val_trg_tensor[:, 1:, :])

        avg_loss = epoch_loss / len(loader)
        print(f"LSTM Epoche {epoch+1}, Train Loss: {avg_loss:.6f}, Val Loss: {val_loss.item():.6f}")
        socketio.emit('training_progress', {'model': 'LSTM', 'epoch': epoch + 1, 'loss': avg_loss, 'val_loss': val_loss.item()})

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            model_version = datetime.now().strftime("%Y%m%d%H%M%S")
            best_model_path = f'lstm_model_v{model_version}.pth'
            best_scaler_path = f'lstm_scaler_v{model_version}.pkl'
            torch.save(model.state_dict(), best_model_path)
            joblib.dump(scaler, best_scaler_path)
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print("Early stopping ausgelöst.")
            break

    return best_model_path, best_scaler_path, best_val_loss.item()


def train_xgboost(df, hparams):
    """Trainiert das XGBoost-Modell mit Validierung."""
    df_with_features = _create_features(df)
    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']
    features = df_with_features[feature_cols].values
    targets = df_with_features['close'].values

    if len(features) < 20:
        print("Nicht genügend Daten für XGBoost-Training.")
        return None, None, float('inf')

    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)
    model = xgb.XGBRegressor(**hparams)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)

    val_score = model.best_score

    model_version = datetime.now().strftime("%Y%m%d%H%M%S")
    model_path = f'xgboost_model_v{model_version}.pkl'
    joblib.dump(model, model_path)
    return model, model_path, val_score

# ==============================================================================
# 6. Hyperparameter-Tuning mit Optuna
# ==============================================================================
def tune_xgboost_hyperparameters(df):
    """Findet die besten Hyperparameter für XGBoost mit Optuna."""
    df_with_features = _create_features(df)
    feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']
    X = df_with_features[feature_cols].values
    y = df_with_features['close'].values
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    def objective(trial):
        params = {
            'objective': 'reg:squarederror',
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        }
        model = xgb.XGBRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)
        return model.best_score

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=20)
    print("Beste XGBoost-Parameter gefunden: ", study.best_params)
    return study.best_params

# ==============================================================================
# 7. Hauptausführung: Training und Laden der Modelle
# ==============================================================================
# Erzeuge eine Dummy-CSV mit mehr Daten für robustes Training
df = pd.DataFrame({
    'date': pd.to_datetime(pd.date_range(start='2022-01-01', periods=500)),
    'open': np.linspace(100, 150, 500) + np.random.randn(500) * 2,
    'high': np.linspace(102, 155, 500) + np.random.randn(500) * 2,
    'low': np.linspace(98, 148, 500) + np.random.randn(500) * 2,
    'close': np.linspace(101, 152, 500) + np.random.randn(500) * 2,
    'volume': np.linspace(10000, 50000, 500) + np.random.randn(500) * 1000
})
df.to_csv('test.csv', index=False)

# --- Hyperparameter-Tuning (optional, aber empfohlen) ---
print("--- Starte Hyperparameter-Tuning für XGBoost ---")
best_xgboost_params = tune_xgboost_hyperparameters(df)
Config.XGBOOST_HPARAMS.update(best_xgboost_params)

# --- Führe das Training für beide Modelle aus ---
print("\n--- Training der Modelle ---")
lstm_model_path, lstm_scaler_path, _ = train_lstm(df, Config.LSTM_HPARAMS)
_, xgboost_model_path, _ = train_xgboost(df, Config.XGBOOST_HPARAMS)
print("\nBeide Modelle wurden trainiert und gespeichert.")

# --- Lade die trainierten Modelle und den Scaler für die Inferenz ---
print("\n--- Lade Modelle für die Inferenz ---")
device = torch.device('cpu')
# KORRIGIERT: Verwende korrekte HPARAMS aus der Config-Klasse
encoder = Encoder(Config.LSTM_HPARAMS['input_dim'], Config.LSTM_HPARAMS['hidden_dim'], Config.LSTM_HPARAMS['n_layers'], Config.LSTM_HPARAMS['dropout_prob']).to(device)
decoder = Decoder(Config.LSTM_HPARAMS['input_dim'], Config.LSTM_HPARAMS['hidden_dim'], Config.LSTM_HPARAMS['n_layers'], Config.LSTM_HPARAMS['dropout_prob']).to(device)
trained_lstm = Seq2Seq(encoder, decoder, device).to(device)
trained_lstm.load_state_dict(torch.load(lstm_model_path))
trained_lstm.eval()
lstm_scaler = joblib.load(lstm_scaler_path)
trained_xgboost = joblib.load(xgboost_model_path)
print("Modelle erfolgreich geladen.")

# ==============================================================================
# 8. Flask API Routen
# ==============================================================================
def calculate_breakout_signal(lstm_preds, ignition_value, current_close, df):
    is_consolidating, low, high = identify_consolidation(
        df,
        'close',
        Config.CONSOLIDATION_BB_WINDOW,
        Config.CONSOLIDATION_LOOKBACK_PERIOD,
        Config.CONSOLIDATION_CHANNEL_THRESHOLD
    )

    projected_close = lstm_preds[-1][3]  # Index 3 ist 'close'
    projected_gain = (projected_close - current_close) / current_close * 100 if current_close > 0 else 0

    signal = "NO_SIGNAL"
    if is_consolidating:
        if ignition_value > current_close and projected_gain >= 30:
            signal = "STRONG_BULLISH_BREAKOUT"
        elif ignition_value < current_close and projected_gain <= -30:
            signal = "STRONG_BEARISH_BREAKOUT"
        else:
            signal = "CONSOLIDATING_NO_BREAKOUT"

    return signal, projected_gain, (low, high) if is_consolidating else (None, None)

@app.route('/predict', methods=['POST'])
def predict_breakout():
    if 'file' not in request.files:
        return jsonify({'error': 'Keine Datei im Request gefunden'}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Keine Datei ausgewählt'}), 400

    if file and file.filename.endswith('.csv'):
        try:
            df = pd.read_csv(file.stream)
        except Exception as e:
            return jsonify({'error': f'Fehler beim Lesen der CSV-Datei: {str(e)}'}), 400

        required_columns = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_columns):
            return jsonify({'error': f'CSV fehlen benötigte Spalten: {required_columns}'}), 400

        # KORRIGIERT: Korrekte GCS-Speichermethode verwenden
        ticker = file.filename.split('.')[0]
        gcs_storage.save_ticker_data(ticker, df.to_dict(orient='records'))

        try:
            sequences = prepare_inference_from_df(df, lstm_scaler, Config.DATA_WINDOW_SIZE)
        except ValueError as e:
             return jsonify({'error': f'Fehler bei der Datenvorbereitung: {e}'}), 400

        if sequences.size == 0:
            return jsonify({'error': 'Nicht genügend Daten in der Datei für eine Vorhersage.'}), 400

        src_tensor_pred = torch.tensor(sequences, dtype=torch.float32).to(device)

        with torch.no_grad():
            lstm_preds = trained_lstm.predict(src_tensor_pred, Config.DATA_PREDICTION_LENGTH)

        unscaled_lstm_preds = lstm_scaler.inverse_transform(lstm_preds.squeeze(0).cpu().numpy())

        df_with_features = _create_features(df.copy())
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'range', 'avg_price', 'rsi', 'macd']
        features = df_with_features[feature_cols].values[-1:]
        xgboost_pred = trained_xgboost.predict(features)[0]

        current_close = df['close'].iloc[-1]
        signal, projected_gain, consolidation_channel = calculate_breakout_signal(unscaled_lstm_preds, xgboost_pred, current_close, df)

        response_data = {
            'lstm_prediction_30_steps': unscaled_lstm_preds.tolist(),
            'xgboost_ignition_value': float(xgboost_pred),
            'signal': signal,
            'projected_gain_percent': projected_gain,
            'current_close': current_close,
            'consolidation_channel': {'low': consolidation_channel[0], 'high': consolidation_channel[1]}
        }
        return jsonify(response_data)

    return jsonify({'error': 'Ungültiger Dateityp, bitte eine .csv-Datei hochladen'}), 400

@app.route('/')
def index():
    return "LSTM & XGBoost Prediction Server is running!"

# ==============================================================================
# 9. Server Start
# ==============================================================================
try:
    ngrok.kill()
except Exception as e:
    print(f"Konnte ngrok-Tunnel nicht beenden: {e}")

public_url = ngrok.connect(5000)
print(f" * Public URL: {public_url}")
print(" * Senden Sie eine POST-Anfrage mit einer CSV-Datei an /predict")

if __name__ == '__main__':
    import threading

    def run_app():
        socketio.run(app, host='0.0.0.0', port=5000)

    flask_thread = threading.Thread(target=run_app)
    flask_thread.daemon = True
    flask_thread.start()
    print(" * Flask-Server läuft in einem Hintergrund-Thread.")