{"cells":[{"cell_type":"code","source":"# project/training.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nimport logging\nimport traceback\nimport copy\n\n# Import from other project modules\nfrom .data_processing import data_quality_check, prepare_sequences\nfrom .features import get_feature_columns, _create_features\nfrom .models import Seq2Seq, Encoder, Decoder, get_device\nfrom .config import Config\n\nlogger = logging.getLogger(__name__)\n\ndef build_and_train_lstm(df, selected_features, hparams, validation_split=0.2):\n    \"\"\"\n    Trainiert das LSTM-Modell mit robusten Methoden zur Vermeidung von Overfitting.\n    Enthält Early Stopping, L2-Regularisierung und reduziertes Teacher Forcing.\n    \"\"\"\n    try:\n        data_quality_check(df)\n        train_df, val_df = train_test_split(df, test_size=validation_split, shuffle=False)\n        \n        src, trg, scaler = prepare_sequences(train_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH, selected_features)\n        val_src, val_trg, _ = prepare_sequences(val_df, Config.DATA_WINDOW_SIZE, Config.DATA_PREDICTION_LENGTH, selected_features, scaler=scaler)\n        \n        if src.shape[0] == 0 or val_src.shape[0] == 0:\n            raise ValueError(\"Not enough data to create train/validation sequences.\")\n        \n        device = get_device()\n        model = Seq2Seq(\n            Encoder(src.shape[-1], hparams['hidden_dim'], hparams['n_layers'], hparams['dropout_prob']),\n            Decoder(src.shape[-1], hparams['hidden_dim'], hparams['n_layers'], hparams['dropout_prob']),\n            device\n        ).to(device)\n        \n        optimizer = optim.Adam(model.parameters(), lr=hparams['learning_rate'], weight_decay=hparams.get('weight_decay', 0))\n        criterion = nn.MSELoss()\n        \n        src_tensor = torch.tensor(src, dtype=torch.float32).to(device)\n        trg_tensor = torch.tensor(trg, dtype=torch.float32).to(device)\n        val_src_tensor = torch.tensor(val_src, dtype=torch.float32).to(device)\n        val_trg_tensor = torch.tensor(val_trg, dtype=torch.float32).to(device)\n\n        best_val_loss = float('inf')\n        patience_counter = 0\n        best_model_state = None\n\n        for epoch in range(hparams['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            output = model(src_tensor, trg_tensor, teacher_forcing_ratio=hparams.get('teacher_forcing_ratio', 0.5))\n            loss = criterion(output, trg_tensor)\n            loss.backward()\n            optimizer.step()\n            \n            # Validation loop\n            model.eval()\n            with torch.no_grad():\n                val_output = model(val_src_tensor, val_trg_tensor, teacher_forcing_ratio=0.0) # Immer ohne TF validieren\n                val_loss = criterion(val_output, val_trg_tensor)\n            \n            if (epoch + 1) % 10 == 0:\n                logger.info(f\"LSTM Epoch {epoch+1}, Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n\n            # Early Stopping\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model_state = copy.deepcopy(model.state_dict())\n                patience_counter = 0\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n                logger.info(f\"Early stopping triggered at epoch {epoch+1}.\")\n                break\n        \n        if best_model_state:\n            model.load_state_dict(best_model_state)\n            \n        return {'model': model, 'scaler': scaler, 'train_loss': loss.item(), 'val_loss': best_val_loss}\n        \n    except Exception as e:\n        logger.error(f\"LSTM training failed: {e}\\n{traceback.format_exc()}\"); raise\n\ndef build_and_train_xgboost(df, hparams, validation_split=0.2):\n    \"\"\"\n    Trainiert das XGBoost-Modell und gibt die Feature Importance zurück.\n    \"\"\"\n    try:\n        data_quality_check(df)\n        df_with_features = _create_features(df.copy())\n        df_with_features['target'] = df_with_features['close'].shift(-1)\n        df_with_features.dropna(inplace=True)\n        if df_with_features.empty:\n            raise ValueError(\"No data left after feature creation.\")\n            \n        X = df_with_features[get_feature_columns()]\n        y = df_with_features['target']\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, shuffle=False)\n        \n        model = xgb.XGBRegressor(**hparams)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)\n        \n        feature_importance = model.get_booster().get_score(importance_type='weight')\n        \n        return {\n            'model': model,\n            'train_score': model.best_score,\n            'feature_importance': feature_importance\n        }\n    except Exception as e:\n        logger.error(f\"XGBoost training failed: {e}\\n{traceback.format_exc()}\"); raise","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}